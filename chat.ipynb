{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdef40f8",
   "metadata": {},
   "source": [
    "# Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc40aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio pandas openai transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ec1a91",
   "metadata": {},
   "source": [
    "# Code Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85700592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from os import listdir, path\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from transformers import pipeline\n",
    "\n",
    "# keeping track of chat histories\n",
    "chat_history_1 = []\n",
    "chat_history_2 = []\n",
    "\n",
    "\n",
    "# counts all numbers of files in a directory\n",
    "def count_files(directory):\n",
    "    return len([f for f in listdir(directory) if path.isfile(path.join(directory, f))])\n",
    "\n",
    "\n",
    "# generates mistral chat response from chat prompts\n",
    "def mistral_response(chats):\n",
    "\n",
    "    try:\n",
    "        # using pipeline\n",
    "        pipe = pipeline(\"text-generation\", \"Open-Orca/Mistral-7B-OpenOrca\")\n",
    "\n",
    "        # getting generated text\n",
    "        generated_text = pipe(chats, max_new_tokens=1000)[0]['generated_text'][-1]\n",
    "\n",
    "        # returning generated text\n",
    "        return generated_text['content']\n",
    "    except Exception:\n",
    "        raise gr.Error(\"Mistral API is running on local machine which encountered an error! Please try again.\")\n",
    "\n",
    "\n",
    "# generates chatgpt api response from chat prompts\n",
    "def chatgpt_api_response(chats):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # need api key from openai\n",
    "        openai_api_key = \"<API KEY>\"\n",
    "\n",
    "        openai_client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "        chatgpt_response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=chats,\n",
    "            max_tokens=300,  # for response length\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.7  # creativity vs. informativeness\n",
    "        )\n",
    "\n",
    "        # return generated text\n",
    "        return chatgpt_response.choices[0].message.content\n",
    "\n",
    "    except Exception:\n",
    "        raise gr.Error(\"ChatGPT API encountered an error! Please try again.\")\n",
    "\n",
    "\n",
    "# gradio block\n",
    "with (gr.Blocks(css=\"style.css\", fill_height=True, title=\"AI Powered Chatbots\") as gradio_interface):\n",
    "\n",
    "    # gradio row\n",
    "    with gr.Row():\n",
    "\n",
    "        # gradio markdown component\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            # AI Powered ChatBot\n",
    "            We are using the Open-Orca/Mistral-7B-OpenOrca fine-tuned model, specifically the Mistral-7B-v0.1 version. We are focusing on using this model to generate SQL queries and later plan to compare the generated responses with those from ChatGPT. For this purpose, please try asking the bot to generate SQL-related responses.\n",
    "            \"\"\")\n",
    "\n",
    "    # gradio row\n",
    "    with gr.Row(elem_id=\"chatbot_container\", equal_height=True):\n",
    "\n",
    "        # gradio column\n",
    "        with gr.Column(scale=1):\n",
    "            chatbox1 = gr.Chatbot(\n",
    "                label=\"Mistral AI Model Response\",\n",
    "                elem_id=\"chatbox_1\",\n",
    "                height=540\n",
    "            )\n",
    "\n",
    "        # gradio column\n",
    "        with gr.Column(scale=1):\n",
    "            chatbox2 = gr.Chatbot(\n",
    "                label=\"ChatGPT API Response\",\n",
    "                elem_id=\"chatbox_2\",\n",
    "                height=540\n",
    "            )\n",
    "\n",
    "    # gradio row\n",
    "    with gr.Row(elem_id=\"input_container\"):\n",
    "\n",
    "        # gradio group\n",
    "        with gr.Group():\n",
    "\n",
    "            # gradio textbox\n",
    "            input_textbox = gr.Textbox(\n",
    "                placeholder=\"Start asking your question...\",\n",
    "                interactive=True,\n",
    "                show_label=False,\n",
    "                lines=2,\n",
    "                autofocus=True,\n",
    "                elem_id=\"text_input\"\n",
    "            )\n",
    "\n",
    "    # gradio row\n",
    "    with gr.Row(elem_id=\"generate_btn_container\"):\n",
    "\n",
    "        # gradio column\n",
    "        with gr.Column(scale=1):\n",
    "            mistral_btn = gr.Button(\"Generate Using Mistral AI\", elem_id=\"mistral_btn\")\n",
    "\n",
    "        # gradio column\n",
    "        with gr.Column(scale=1):\n",
    "            chatgpt_btn = gr.Button(\"Generate Using ChatGPT API\", elem_id=\"chatgpt_btn\")\n",
    "\n",
    "    # gradio row\n",
    "    with gr.Row(elem_id=\"tool_btn_container\"):\n",
    "\n",
    "        # gradio column\n",
    "        with gr.Column(scale=1):\n",
    "            clear_btn = gr.Button(\"Clear Everything\", elem_id=\"clear_btn\")\n",
    "\n",
    "        # gradio column\n",
    "        with gr.Column(scale=1):\n",
    "            save_btn = gr.Button(\"Save Responses to CSV by Pair\", elem_id=\"save_btn\")\n",
    "\n",
    "    # prepares mistral start ui\n",
    "    def mistral_start():\n",
    "        return gr.Button(interactive=False)\n",
    "\n",
    "    # resets views when mistral response generation is finish\n",
    "    def mistral_end():\n",
    "        return gr.Button(interactive=True)\n",
    "\n",
    "    # prepares chatgpt start ui\n",
    "    def chatgpt_start():\n",
    "        return gr.Button(interactive=False)\n",
    "\n",
    "    # resets views when chatgpt response generation is finish\n",
    "    def chatgpt_end():\n",
    "        return gr.Button(interactive=True)\n",
    "\n",
    "    # handles mistral bot request\n",
    "    def mistral_bot_handler(message, history):\n",
    "\n",
    "        # check and clear history if needed\n",
    "        if len(history) == 0:\n",
    "            chat_history_1.clear()\n",
    "\n",
    "        # updating formatted chat history\n",
    "        chat_history_1.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        # return message and history\n",
    "        return \"\", history + [[message, None]]\n",
    "\n",
    "    # handles chatgpt bot request\n",
    "    def chatbot_bot_handler(message, history):\n",
    "\n",
    "        # check and clear history if needed\n",
    "        if len(history) == 0:\n",
    "            chat_history_2.clear()\n",
    "\n",
    "        # updating formatted chat history\n",
    "        chat_history_2.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        # return message and history\n",
    "        return \"\", history + [[message, None]]\n",
    "\n",
    "    # calls mistral generation function and handles final response\n",
    "    def mistral_response_handler(history):\n",
    "\n",
    "        # generating text using mistral\n",
    "        generated_text = mistral_response(chat_history_1)\n",
    "\n",
    "        # updating formatted chat history\n",
    "        chat_history_1.append({\"role\": \"assistant\", \"content\": generated_text})\n",
    "\n",
    "        try:\n",
    "            # preparing generated text for streaming response\n",
    "            history[-1][1] = \"\"\n",
    "            for character in generated_text:\n",
    "                history[-1][1] += character\n",
    "                time.sleep(0.01)\n",
    "                yield history\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # calls chatgpt generation function and handles final response\n",
    "    def chatgpt_response_handler(history):\n",
    "\n",
    "        # getting generated text from chatgpt\n",
    "        generated_text = chatgpt_api_response(chat_history_2)\n",
    "\n",
    "        # updating formatted chat history\n",
    "        chat_history_2.append({\"role\": \"assistant\", \"content\": generated_text})\n",
    "\n",
    "        # preparing generated text for streaming response\n",
    "        history[-1][1] = \"\"\n",
    "        for character in generated_text:\n",
    "            history[-1][1] += character\n",
    "            time.sleep(0.01)\n",
    "            yield history\n",
    "\n",
    "    # function to validate text input\n",
    "    def validate_textbox(message, history):\n",
    "\n",
    "        # making sure message is not empty\n",
    "        if len(message) < 1:\n",
    "            raise gr.Error(\"Please enter your question in the textbox!\")\n",
    "\n",
    "        # making sure same input is not being entered multiple time\n",
    "        if len(history) > 0 and history[-1][0] == message:\n",
    "            raise gr.Error(\"Please enter a different prompt!\")\n",
    "\n",
    "\n",
    "    # on click mistral generate button call functions sequentially\n",
    "    mistral_btn.click(\n",
    "\n",
    "        # validate text input\n",
    "        validate_textbox, [input_textbox, chatbox1]).success(\n",
    "\n",
    "        # initiate mistral start ui change\n",
    "        mistral_start, outputs=mistral_btn\n",
    "    ).success(\n",
    "\n",
    "        # initiate bot handler\n",
    "        mistral_bot_handler, [input_textbox, chatbox1], [input_textbox, chatbox1], queue=False\n",
    "    ).success(\n",
    "\n",
    "        # get response and fill chat box\n",
    "        mistral_response_handler, chatbox1, chatbox1\n",
    "    ).success(\n",
    "\n",
    "        # reset ui change\n",
    "        mistral_end, outputs=mistral_btn\n",
    "    )\n",
    "\n",
    "    # on click chatgpt generate button call functions sequentially\n",
    "    chatgpt_btn.click(\n",
    "\n",
    "        # validate text input\n",
    "        validate_textbox, [input_textbox, chatbox2]).success(\n",
    "\n",
    "        # initiate mistral start ui change\n",
    "        chatgpt_start, outputs=chatgpt_btn\n",
    "    ).success(\n",
    "\n",
    "        # initiate bot handler\n",
    "        chatbot_bot_handler, [input_textbox, chatbox2], [input_textbox, chatbox2], queue=False\n",
    "    ).success(\n",
    "\n",
    "        # get response and fill chat box\n",
    "        chatgpt_response_handler, chatbox2, chatbox2\n",
    "    ).success(\n",
    "        chatgpt_end, outputs=chatgpt_btn\n",
    "    )\n",
    "\n",
    "    # clear history\n",
    "    def clear_history():\n",
    "\n",
    "        # remove chat histories\n",
    "        chat_history_1.clear()\n",
    "        chat_history_2.clear()\n",
    "        return \"\", [], []\n",
    "\n",
    "    # function for formatting purposes to save data into files\n",
    "    def get_data_by_key(data, key):\n",
    "        for entry in data:\n",
    "            if entry[0] == key:\n",
    "                return entry[1]\n",
    "            return None\n",
    "\n",
    "    # function to save data into files\n",
    "    def save_history(history1, history2):\n",
    "\n",
    "        # save only if we have history entry\n",
    "        if len(history1) < 1:\n",
    "            raise gr.Error(\n",
    "                \"The conversation history is empty! Please make some conversation with both bots and try again!\")\n",
    "\n",
    "        # will store formatted chat history here\n",
    "        formatted_chat_history = []\n",
    "\n",
    "        # looping through history\n",
    "        for item1 in chat_history_1:\n",
    "\n",
    "            # getting user prompt\n",
    "            if item1['role'] == 'user':\n",
    "\n",
    "                # looping through chat history 2\n",
    "                for item2 in chat_history_2:\n",
    "\n",
    "                    # making sure to pick content that matches in both list\n",
    "                    if item2['role'] == 'user' and item1['content'] == item2['content']:\n",
    "                        # getting the mistral response from history\n",
    "                        m_response = chat_history_1[chat_history_1.index(item1) + 1]['content'] if len(\n",
    "                            chat_history_1) > chat_history_1.index(item1) + 1 else ''\n",
    "\n",
    "                        # getting the chatgpt response from history\n",
    "                        c_response = chat_history_2[chat_history_2.index(item2) + 1]['content'] if len(\n",
    "                            chat_history_2) > chat_history_2.index(item2) + 1 else ''\n",
    "\n",
    "                        # appending into list\n",
    "                        formatted_chat_history.append({\n",
    "                            'question': item1['content'],\n",
    "                            'mistral_response': m_response,\n",
    "                            'chatgpt_response': c_response\n",
    "                        })\n",
    "\n",
    "        # creating a Pandas DataFrame from the list of dictionaries\n",
    "        df = pd.DataFrame(formatted_chat_history)\n",
    "\n",
    "        # setting filename to number of files +1\n",
    "        filename = count_files(\"GeneratedData/csv\") + 1\n",
    "\n",
    "        # Save the DataFrame to a CSV file with index=False to avoid an extra index column\n",
    "        df.to_csv(f\"GeneratedData/csv/{filename}.csv\", index=False)\n",
    "        df.to_json(f\"GeneratedData/json/{filename}.json\", index=False)\n",
    "\n",
    "\n",
    "    # on click event for clear button\n",
    "    clear_btn.click(clear_history, outputs=[input_textbox, chatbox1, chatbox2], queue=False)\n",
    "\n",
    "    # on click event for save button\n",
    "    save_btn.click(save_history, inputs=[chatbox1, chatbox2], queue=False).success(\n",
    "        lambda text=\"File saved successfully!\": gr.Info(text), inputs=None, outputs=None\n",
    "    ).success(\n",
    "        clear_history, outputs=[input_textbox, chatbox1, chatbox2], queue=False\n",
    "    ).success(\n",
    "        lambda text=\"History has been cleared automatically for new prompts!\": gr.Info(text), inputs=None, outputs=None\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # using queue for streaming chat\n",
    "    gradio_interface.queue()\n",
    "\n",
    "    # launching gradio interface\n",
    "    gradio_interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a380e1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
